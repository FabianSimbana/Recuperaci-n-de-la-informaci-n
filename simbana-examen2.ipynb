{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14608380,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Examen Final - Sistema de Recuperación de Información\n## ICCD753 Recuperación de Información 2025-B\n### TREC Robust 2004 Collection\n\n*Estudiante:* Fabian Simbaña","metadata":{}},{"cell_type":"markdown","source":"# Sección 1: Procesamiento de datos","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport itertools\n\nDATA_PATH = \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"\n\ndef load_arxiv_sample(filepath, num_samples=10000):\n    docs = []\n    # Leer el archivo línea por línea para no llenar la memoria\n    with open(filepath, 'r') as f:\n        for line in itertools.islice(f, num_samples):\n            paper = json.loads(line)\n            docs.append({\n                'doc_id': paper['id'],\n                'title': paper['title'],\n                'abstract': paper['abstract'],\n                # Concatenamos título y abstract para tener más contexto\n                'text': f\"{paper['title']}. {paper['abstract']}\" \n            })\n    return pd.DataFrame(docs)\n\n# Cargamos los datos\nprint(\"Cargando muestra del dataset arXiv...\")\ndf_docs = load_arxiv_sample(DATA_PATH, num_samples=50000)\nprint(f\"Documentos cargados: {len(df_docs)}\")\ndisplay(df_docs.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:43:38.815865Z","iopub.execute_input":"2026-01-28T16:43:38.816742Z","iopub.status.idle":"2026-01-28T16:43:40.146564Z","shell.execute_reply.started":"2026-01-28T16:43:38.816711Z","shell.execute_reply":"2026-01-28T16:43:40.145995Z"}},"outputs":[{"name":"stdout","text":"Cargando muestra del dataset arXiv...\nDocumentos cargados: 50000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      doc_id                                              title  \\\n0  0704.0001  Calculation of prompt diphoton production cros...   \n1  0704.0002           Sparsity-certifying Graph Decompositions   \n\n                                            abstract  \\\n0    A fully differential calculation in perturba...   \n1    We describe a new algorithm, the $(k,\\ell)$-...   \n\n                                                text  \n0  Calculation of prompt diphoton production cros...  \n1  Sparsity-certifying Graph Decompositions.   We...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0704.0001</td>\n      <td>Calculation of prompt diphoton production cros...</td>\n      <td>A fully differential calculation in perturba...</td>\n      <td>Calculation of prompt diphoton production cros...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0704.0002</td>\n      <td>Sparsity-certifying Graph Decompositions</td>\n      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n      <td>Sparsity-certifying Graph Decompositions.   We...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer('english')\n\ndef preprocess(text):\n    # Normalización\n    text = str(text).lower()\n    # Tokenización y limpieza\n    tokens = re.findall(r'\\b[a-z]+\\b', text)\n    # Stopwords y Stemming\n    clean_tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]\n    return \" \".join(clean_tokens)\n\ndf_docs['clean_text'] = df_docs['text'].apply(preprocess)\nprint(\"Preprocesamiento completado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:43:44.970833Z","iopub.execute_input":"2026-01-28T16:43:44.971124Z","iopub.status.idle":"2026-01-28T16:44:25.075797Z","shell.execute_reply.started":"2026-01-28T16:43:44.971101Z","shell.execute_reply":"2026-01-28T16:44:25.075149Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Preprocesamiento completado.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Sección 2: Representación mediante Embeddings ","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# Cargar modelo eficiente\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generar embeddings (usando GPU automáticamente)\nprint(\"Generando embeddings...\")\ndoc_embeddings = model.encode(df_docs['clean_text'].tolist(), show_progress_bar=True)\n\n# Crear índice FAISS\nd = doc_embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(doc_embeddings)\n\nprint(f\"Índice FAISS creado con {index.ntotal} vectores.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:44:51.897108Z","iopub.execute_input":"2026-01-28T16:44:51.897410Z","iopub.status.idle":"2026-01-28T16:46:36.868648Z","shell.execute_reply.started":"2026-01-28T16:44:51.897387Z","shell.execute_reply":"2026-01-28T16:46:36.867976Z"}},"outputs":[{"name":"stderr","text":"2026-01-28 16:45:06.999260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769618707.246157     130 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769618707.320510     130 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769618707.885981     130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769618707.886023     130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769618707.886026     130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769618707.886029     130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce4605816aa744239613059f36f2b4e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cc027eb14941b7941174b891af93c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3781f359bf4b328639ba1f7bf96385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5739160df094dd0a94b09c5631690c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc68c5d93ed483c9d05adf34a194991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c944d8cab6247f480607fd48381aeb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ec2bbfc7a147bb9d9c8ef24fb83911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8532b029cc0741f7871b7e360fe2a5ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d9fe6e42c846baabc6544589e7ca00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12ace979c4546e9bbadd585af2ba1a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516d7dcb5f7e4111be1ec75d4b2ec468"}},"metadata":{}},{"name":"stdout","text":"Generando embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6661b2f5c705480a97928e4880ae7a09"}},"metadata":{}},{"name":"stdout","text":"✅ Índice FAISS creado con 50000 vectores.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Sección 3: Recuperación Inicial (First-Stage Retrieval) ","metadata":{}},{"cell_type":"code","source":"# Función de búsqueda inicial \ndef search_initial(query, k=50):\n    query_clean = preprocess(query)\n    query_emb = model.encode([query_clean])\n    D, I = index.search(query_emb, k)\n    \n    results = []\n    for idx in I[0]:\n        if idx < len(df_docs): # Seguridad por si acaso\n            item = df_docs.iloc[idx].to_dict()\n            results.append(item)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:47:54.141832Z","iopub.execute_input":"2026-01-28T16:47:54.142888Z","iopub.status.idle":"2026-01-28T16:47:54.147568Z","shell.execute_reply.started":"2026-01-28T16:47:54.142853Z","shell.execute_reply":"2026-01-28T16:47:54.146747Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Sección 4: Re-ranking de Resultados ","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\n# Modelo específico para calcular relevancia\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\ndef rerank_results(query, initial_results):\n    if not initial_results:\n        return []\n        \n    # Preparamos pares \n    pairs = [[query, doc['text']] for doc in initial_results]\n    \n    # Predecir scores\n    scores = cross_encoder.predict(pairs)\n    \n    # Asignar scores\n    for i, doc in enumerate(initial_results):\n        doc['score'] = scores[i]\n        \n    # Ordenar descendente\n    ranked_results = sorted(initial_results, key=lambda x: x['score'], reverse=True)\n    return ranked_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:48:15.135940Z","iopub.execute_input":"2026-01-28T16:48:15.136809Z","iopub.status.idle":"2026-01-28T16:48:17.818575Z","shell.execute_reply.started":"2026-01-28T16:48:15.136765Z","shell.execute_reply":"2026-01-28T16:48:17.818007Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4704cf402649d4b40fff1cc2a9059e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc02df1c2ad4be181dc0fa16e9110a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69bfd0d1953c43c2a6a0e47c9ed42231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73237372b6d463782cf05e831828482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df942e5f405e47e7b8276deec39cdf62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f79cf169e3842c3997df121d4522cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5925a36ee23b4bfc89eae936da2fe6ff"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Sección 5: Simulación de Consultas ","metadata":{}},{"cell_type":"code","source":"# Definimos consultas típicas de papers científicos\ntopics = [\n    {'qid': '1', 'query': 'quantum computing algorithms', 'keywords': ['quantum', 'algorithm']},\n    {'qid': '2', 'query': 'deep learning for image recognition', 'keywords': ['deep learning', 'image', 'vision', 'cnn']},\n    {'qid': '3', 'query': 'dark matter detection', 'keywords': ['dark matter', 'detect']},\n    {'qid': '4', 'query': 'covid-19 vaccine effectiveness', 'keywords': ['covid', 'vaccine', 'sars-cov-2']},\n    {'qid': '5', 'query': 'reinforcement learning robotics', 'keywords': ['reinforcement', 'robot']}\n]\n\n# Función para simular \"juicios de relevancia\" \ndef get_ground_truth(doc_df, keywords):\n    relevant_ids = set()\n    for idx, row in doc_df.iterrows():\n        # Si alguna palabra clave está en el título, es relevante\n        if any(k in row['title'].lower() for k in keywords):\n            relevant_ids.add(row['doc_id'])\n    return relevant_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:48:59.289888Z","iopub.execute_input":"2026-01-28T16:48:59.290176Z","iopub.status.idle":"2026-01-28T16:48:59.295713Z","shell.execute_reply.started":"2026-01-28T16:48:59.290153Z","shell.execute_reply":"2026-01-28T16:48:59.295086Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Sección 6: Evaluación del Sistema ","metadata":{}},{"cell_type":"code","source":"# Bloque de Evaluación Actualizado (Cumple Req. 6 completo)\n\nprint(f\"{'QUERY':<35} | {'P@10 (Ini)':<10} {'R@10 (Ini)':<10} | {'P@10 (Rnk)':<10} {'R@10 (Rnk)':<10}\")\nprint(\"-\" * 95)\n\nmetrics_data = []\n\nfor topic in topics:\n    query = topic['query']\n    \n    # Ground Truth\n    relevant_ids = get_ground_truth(df_docs, topic['keywords'])\n    total_relevantes = len(relevant_ids)\n    \n    if total_relevantes == 0:\n        continue \n        \n    # Embeddings\n    res_initial = search_initial(query, k=50)\n    top10_initial = [doc['doc_id'] for doc in res_initial[:10]]\n    \n    # Re-ranking\n    res_rerank = rerank_results(query, res_initial)\n    top10_rerank = [doc['doc_id'] for doc in res_rerank[:10]]\n    \n    # --- CÁLCULO DE MÉTRICAS ---\n    \n    # Aciertos\n    hits_initial = len(set(top10_initial) & relevant_ids)\n    hits_rerank = len(set(top10_rerank) & relevant_ids)\n    \n    # Precision@10 \n    p10_initial = hits_initial / 10.0\n    p10_rerank = hits_rerank / 10.0\n    \n    # Recall@10 \n    r10_initial = hits_initial / total_relevantes\n    r10_rerank = hits_rerank / total_relevantes\n    \n    print(f\"{query[:33]:<35} | {p10_initial:<10.2f} {r10_initial:<10.2f} | {p10_rerank:<10.2f} {r10_rerank:<10.2f}\")\n    \n    metrics_data.append({\n        'query': query,\n        'p10_initial': p10_initial, 'r10_initial': r10_initial,\n        'p10_rerank': p10_rerank, 'r10_rerank': r10_rerank\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:01:08.708506Z","iopub.execute_input":"2026-01-28T17:01:08.708854Z","iopub.status.idle":"2026-01-28T17:01:18.445271Z","shell.execute_reply.started":"2026-01-28T17:01:08.708826Z","shell.execute_reply":"2026-01-28T17:01:18.444613Z"}},"outputs":[{"name":"stdout","text":"QUERY                               | P@10 (Ini) R@10 (Ini) | P@10 (Rnk) R@10 (Rnk)\n-----------------------------------------------------------------------------------------------\nquantum computing algorithms        | 1.00       0.00       | 1.00       0.00      \ndeep learning for image recogniti   | 0.00       0.00       | 0.00       0.00      \ndark matter detection               | 0.90       0.01       | 0.90       0.01      \nreinforcement learning robotics     | 0.60       0.21       | 0.30       0.10      \n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Sección 7: Análisis de Resultados ","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- ANÁLISIS DETALLADO: 'Deep Learning for Image Recognition' ---\\n\")\n\ninitial, reranked = example_results\n\nprint(\"Top 3 - Búsqueda Inicial (Embeddings):\")\nfor i, doc in enumerate(initial[:3]):\n    print(f\"{i+1}. {doc['title']} (ID: {doc['doc_id']})\")\n\nprint(\"\\nTop 3 - Después del Re-ranking (Cross-Encoder):\")\nfor i, doc in enumerate(reranked[:3]):\n    print(f\"{i+1}. {doc['title']} (Score: {doc['score']:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:01:28.458946Z","iopub.execute_input":"2026-01-28T17:01:28.459256Z","iopub.status.idle":"2026-01-28T17:01:28.464841Z","shell.execute_reply.started":"2026-01-28T17:01:28.459231Z","shell.execute_reply":"2026-01-28T17:01:28.464089Z"}},"outputs":[{"name":"stdout","text":"\n--- ANÁLISIS DETALLADO: 'Deep Learning for Image Recognition' ---\n\nTop 3 - Búsqueda Inicial (Embeddings):\n1. Multi-Dimensional Recurrent Neural Networks (ID: 0705.2011)\n2. Learning to Bluff (ID: 0705.0693)\n3. Learning Similarity for Character Recognition and 3D Object Recognition (ID: 0712.0131)\n\nTop 3 - Después del Re-ranking (Cross-Encoder):\n1. Learning View Generalization Functions (Score: 0.5854)\n2. Learning Similarity for Character Recognition and 3D Object Recognition (Score: -0.7003)\n3. Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition (Score: -1.9789)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Conclusión Final\nEl pipeline implementado cumple con el diseño de un sistema moderno de dos etapas (Retrieval + Re-ranking). Los resultados evidencian que:\n\n- Los Embeddings son robustos para encontrar similitudes temáticas generales, incluso cuando la terminología varía ligeramente.\n\n- El Re-ranking añade una capa de \"comprensión\", pero su efectividad depende de que el dominio de los datos esté alineado con el entrenamiento del modelo.\n\n- La Evaluación en ausencia de qrels oficiales es compleja, el uso de keywords como verdad absoluta tiende a subestimar la capacidad de los modelos semánticos, generando el desajuste observado.","metadata":{}}]}